{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 – Exploratory Analysis (Python)\n",
    "\n",
    "## 1. Data Understanding & Cleaning\n",
    "- Load the datasets\n",
    "- Handle missing values or outliers if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users shape: (121, 4)\n",
      "    user_id signup_date         region  platform\n",
      "0  739b720a  2023-04-21  United States       iOS\n",
      "1  0aa3325c  2023-06-12             US       iOS\n",
      "2  b5a2048e  2023-08-20             KR       Web\n",
      "3  a4a70166  2023-05-26            can  3rdParty\n",
      "4  2386e2a0  2023-01-18            USA   Android \n",
      "\n",
      "         user_id signup_date         region platform\n",
      "count        121         121            121      121\n",
      "unique       120         103              8        4\n",
      "top     739b720a  2023-04-21  United States      iOS\n",
      "freq           2           2             18       32 \n",
      "\n",
      "Missing values (Users):\n",
      "user_id        0\n",
      "signup_date    0\n",
      "region         0\n",
      "platform       0\n",
      "dtype: int64\n",
      "\n",
      "Unique Regions:\n",
      "['United States' 'US' 'KR' 'can' 'USA' 'CA' 'JP' 'Canada']\n",
      "\n",
      "Unique Platforms:\n",
      "['iOS' 'Web' '3rdParty' 'Android']\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "df_events = pd.read_csv('../data/events.csv')\n",
    "df_devices = pd.read_csv('../data/devices.csv')\n",
    "df_users = pd.read_csv('../data/users.csv')\n",
    "# number of rows and columns.\n",
    "print(\"Users shape:\", df_users.shape)\n",
    "# Print first 5 rows to understand the data structure\n",
    "print(df_users.head(),\"\\n\")\n",
    "\n",
    "# statistical summary of the dataset\n",
    "print(df_users.describe(),\"\\n\")\n",
    "\n",
    "print(\"Missing values (Users):\")\n",
    "print(df_users.isnull().sum())\n",
    "# Unique values for categorical columns\n",
    "print(\"\\nUnique Regions:\")\n",
    "print(df_users[\"region\"].unique())\n",
    "\n",
    "print(\"\\nUnique Platforms:\")\n",
    "print(df_users[\"platform\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devices shape: (450, 6)\n",
      "    device_id   user_id network device_type firmware_version location\n",
      "0  d_f4abcb9e  89a25ef6    tuya         fan            1.0.3   Office\n",
      "1  d_212b804b  2f555493    tuya  smart_plug            1.0.0      NaN\n",
      "2  d_32b272b3  f41317b2    tuya      heater         3.5.beta      NaN\n",
      "3  d_f6fe781d  ee413bc3    tuya         fan         3.5.beta      NaN\n",
      "4  d_8555efd8  cd6e76a2    tuya  smart_bulb         3.5.beta     Home \n",
      "\n",
      "         device_id   user_id network device_type firmware_version location\n",
      "count          450       425     450         450              450      342\n",
      "unique         450       119       2           8                4        3\n",
      "top     d_9f01e3d6  739b720a    tuya  thermostat            2.1.0     Home\n",
      "freq             1        10     228          70              115      136 \n",
      "\n",
      "Missing values (Devices):\n",
      "device_id             0\n",
      "user_id              25\n",
      "network               0\n",
      "device_type           0\n",
      "firmware_version      0\n",
      "location            108\n",
      "dtype: int64\n",
      "\n",
      "Unique Networks:\n",
      "['tuya' 'ayla']\n",
      "\n",
      "Unique Device Types:\n",
      "['fan' 'smart_plug' 'heater' 'smart_bulb' 'door_sensor' 'legrand_switch'\n",
      " 'thermostat' 'window_sensor']\n",
      "\n",
      "Unique Firmware Versions:\n",
      "['1.0.3' '1.0.0' '3.5.beta' '2.1.0']\n",
      "\n",
      "Unique Locations:\n",
      "['Office' nan 'Home' 'Cottage']\n",
      "Devices location nulls filled: 108\n",
      "Devices with no user_id flagged: 25\n",
      "Devices on beta firmware: 115\n"
     ]
    }
   ],
   "source": [
    "# number of rows and columns.\n",
    "print(\"devices shape:\", df_devices.shape)\n",
    "# Print first 5 rows to understand the data structure\n",
    "print(df_devices.head(), \"\\n\")\n",
    "\n",
    "# statistical summary of the dataset\n",
    "print(df_devices.describe(), \"\\n\")\n",
    "\n",
    "print(\"Missing values (Devices):\")\n",
    "print(df_devices.isnull().sum())\n",
    "# Unique values for categorical columns\n",
    "\n",
    "print(\"\\nUnique Networks:\")\n",
    "print(df_devices[\"network\"].unique())\n",
    "\n",
    "print(\"\\nUnique Device Types:\")\n",
    "print(df_devices[\"device_type\"].unique())\n",
    "\n",
    "print(\"\\nUnique Firmware Versions:\")\n",
    "print(df_devices[\"firmware_version\"].unique())\n",
    "\n",
    "print(\"\\nUnique Locations:\")\n",
    "print(df_devices[\"location\"].unique())\n",
    "# Clean missing location to Unknown and\n",
    "df_devices[\"location\"] = df_devices[\"location\"].fillna(\"Unknown\")\n",
    "df_devices[\"user_id_missing\"] = df_devices[\"user_id\"].isnull()\n",
    "print(f\"Devices location nulls filled: {(df_devices['location'] == 'Unknown').sum()}\")\n",
    "print(f\"Devices with no user_id flagged: {df_devices['user_id_missing'].sum()}\")\n",
    "print(f\"Devices on beta firmware: {(df_devices['firmware_version'] == '3.5.beta').sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events shape: (15000, 6)\n",
      "  event_id   device_id event_type  event_value                  event_ts  \\\n",
      "0      e_0  d_87cfe321  telemetry  see_payload  2024-01-18T17:45:43.000Z   \n",
      "1      e_1  d_fddd9d33  telemetry  see_payload  2024-02-04T16:18:19.000Z   \n",
      "2      e_2  d_c5859fb6  telemetry  see_payload  2024-01-12T10:57:04.000Z   \n",
      "3      e_3  d_de3f2763  telemetry  see_payload  2024-01-02T08:09:18.000Z   \n",
      "4      e_4  d_8406c179  telemetry  see_payload  2024-01-23T01:15:16.000Z   \n",
      "\n",
      "                                             payload  \n",
      "0  {\"metadata\": {\"oem_model\": \"door_sensor\", \"dsn...  \n",
      "1  {\"metadata\": {\"oem_model\": \"window_sensor\", \"d...  \n",
      "2  {\"status\": [{\"code\": \"generic_state\", \"value\":...  \n",
      "3  {\"status\": [{\"code\": \"switch_led\", \"value\": fa...  \n",
      "4  {\"status\": [{\"code\": \"generic_state\", \"value\":...   \n",
      "\n",
      "       event_id   device_id event_type  event_value                  event_ts  \\\n",
      "count     15000       15000      15000        15000                     15000   \n",
      "unique    15000         450          1            1                     14976   \n",
      "top     e_14999  d_f4abcb9e  telemetry  see_payload  2024-01-12T15:49:47.000Z   \n",
      "freq          1        1517      15000        15000                         2   \n",
      "\n",
      "                                                  payload  \n",
      "count                                               15000  \n",
      "unique                                              10002  \n",
      "top     {\"status\": [{\"code\": \"generic_state\", \"value\":...  \n",
      "freq                                                 4999   \n",
      "\n",
      "Missing values (Events):\n",
      "event_id       0\n",
      "device_id      0\n",
      "event_type     0\n",
      "event_value    0\n",
      "event_ts       0\n",
      "payload        0\n",
      "dtype: int64\n",
      "── Payload A ──\n",
      "{\n",
      "  \"metadata\": {\n",
      "    \"oem_model\": \"door_sensor\",\n",
      "    \"dsn\": \"ACD6F3E680-6\"\n",
      "  },\n",
      "  \"datapoint\": {\n",
      "    \"property\": \"contact_state\",\n",
      "    \"value\": 1,\n",
      "    \"echo\": false\n",
      "  }\n",
      "}\n",
      "── Payload B ──\n",
      "{\n",
      "  \"status\": [\n",
      "    {\n",
      "      \"code\": \"generic_state\",\n",
      "      \"value\": \"online\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Device A network: ayla\n",
      "Device B network: tuya\n"
     ]
    }
   ],
   "source": [
    "# number of rows and columns.\n",
    "print(\"events shape:\", df_events.shape)\n",
    "# Print first 5 rows to understand the data structure\n",
    "print(df_events.head(), \"\\n\")\n",
    "# summary\n",
    "print(df_events.describe(), \"\\n\")\n",
    "\n",
    "print(\"Missing values (Events):\")\n",
    "print(df_events.isnull().sum())\n",
    "\n",
    "import json\n",
    "# get two raw payloads with different structures\n",
    "sample_metadata = df_events[df_events[\"payload\"].str.contains('\"metadata\"')].iloc[0]\n",
    "sample_status = df_events[df_events[\"payload\"].str.contains('\"status\"')].iloc[0]\n",
    "\n",
    "print(\"── Payload A ──\")\n",
    "print(json.dumps(json.loads(sample_metadata[\"payload\"]), indent=2))\n",
    "print(\"── Payload B ──\")\n",
    "print(json.dumps(json.loads(sample_status[\"payload\"]), indent=2))\n",
    "# See what network these devices actually belong to\n",
    "device_a = df_devices[df_devices[\"device_id\"] == sample_metadata[\"device_id\"]][\n",
    "    [\"device_id\", \"network\"]\n",
    "]\n",
    "device_b = df_devices[df_devices[\"device_id\"] == sample_status[\"device_id\"]][\n",
    "    [\"device_id\", \"network\"]\n",
    "]\n",
    "\n",
    "print(\"\\nDevice A network:\", device_a[\"network\"].values[0])\n",
    "print(\"Device B network:\", device_b[\"network\"].values[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate user_ids dropped: 1\n",
      "Users cleaned: (120, 4)\n",
      "Region counts after normalization:\n",
      "region\n",
      "US    47\n",
      "CA    45\n",
      "KR    17\n",
      "JP    11\n"
     ]
    }
   ],
   "source": [
    "# Cleaning users.csv\n",
    "# only US and Canada have the messy variants. Any region not in region_map will be set to Other\n",
    "region_map = {\n",
    "    \"United States\": \"US\",\n",
    "    \"USA\": \"US\",\n",
    "    \"US\": \"US\",\n",
    "    \"Canada\": \"CA\",\n",
    "    \"can\": \"CA\",\n",
    "    \"CA\": \"CA\",\n",
    "    \"KR\": \"KR\",\n",
    "    \"JP\": \"JP\",\n",
    "}\n",
    "df_users[\"region\"] = df_users[\"region\"].map(region_map).fillna(\"Other\")\n",
    "\n",
    "# Drop duplicate user_id\n",
    "dupes = df_users[\"user_id\"].duplicated().sum()\n",
    "print(f\"\\nDuplicate user_ids dropped: {dupes}\")\n",
    "df_users = df_users.drop_duplicates(subset=\"user_id\", keep=\"first\")\n",
    "\n",
    "# Parse signup_date to datetime\n",
    "df_users[\"signup_date\"] = pd.to_datetime(df_users[\"signup_date\"])\n",
    "\n",
    "print(\"Users cleaned:\", df_users.shape)\n",
    "print(\"Region counts after normalization:\")\n",
    "print(df_users[\"region\"].value_counts().to_string(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing payload column (15,000 rows)...\n",
      "✓ Events cleaned: (15000, 18)\n",
      "\n",
      "Payload source breakdown:\n",
      "source\n",
      "tuya    8319\n",
      "ayla    6681\n",
      "\n",
      "Top extracted codes:\n",
      "extracted_code\n",
      "generic_state        4999\n",
      "contact_state        3228\n",
      "local_temperature    2092\n",
      "switch_led           1890\n",
      "switch_1             1430\n",
      "connectivity         1361\n",
      "\n",
      "All unique extracted codes:\n",
      "extracted_code\n",
      "generic_state        4999\n",
      "contact_state        3228\n",
      "local_temperature    2092\n",
      "switch_led           1890\n",
      "switch_1             1430\n",
      "connectivity         1361\n",
      "\n",
      "Dedicated measurement columns:\n",
      "  voltage_v readings:  1430\n",
      "  current_a readings:  1430\n",
      "  power_w readings:    1430\n",
      "  temperature_c readings: 2092\n",
      "network  source\n",
      "ayla     ayla      6681\n",
      "tuya     tuya      8319\n",
      "dtype: int64\n",
      "\n",
      "─── Final Dataset Summary ───\n",
      "Users:   120 rows | Regions: ['CA', 'JP', 'KR', 'US']\n",
      "Devices: 450 rows | Networks: ['tuya', 'ayla']\n",
      "Events:  15000 rows | Date range: 2024-01-01 → 2024-02-07\n"
     ]
    }
   ],
   "source": [
    "# event_value is always \"see_payload\" real data in payload\n",
    "\n",
    "\n",
    "def parse_payload(raw):\n",
    "    try:\n",
    "        data = json.loads(raw)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return pd.Series({\n",
    "            'source':           'parse_error',\n",
    "            'extracted_code':   None,\n",
    "            'extracted_value':  None,\n",
    "            'voltage':          None,\n",
    "            'current':          None,\n",
    "            'power':            None,\n",
    "            'temperature_c':    None,\n",
    "        })\n",
    "\n",
    "    # ── Ayla structure ──\n",
    "    if 'metadata' in data and 'datapoint' in data:\n",
    "        prop  = data['datapoint'].get('property')\n",
    "        value = data['datapoint'].get('value')\n",
    "        return pd.Series({\n",
    "            'source':           'ayla',\n",
    "            'extracted_code':   prop,\n",
    "            'extracted_value':  value,\n",
    "            'voltage':          None,\n",
    "            'current':          None,\n",
    "            'power':            None,\n",
    "            'temperature_c':    value if prop == 'local_temperature' else None,\n",
    "        })\n",
    "\n",
    "    # ── Tuya structure ──\n",
    "    if 'status' in data and isinstance(data['status'], list) and data['status']:\n",
    "        # index all codes in this event\n",
    "        status_map = {item['code']: item['value'] for item in data['status']}\n",
    "\n",
    "        # primary code is still the first one (for extracted_code/value)\n",
    "        first = data['status'][0]\n",
    "\n",
    "        return pd.Series({\n",
    "            'source':           'tuya',\n",
    "            'extracted_code':   first.get('code'),\n",
    "            'extracted_value':  first.get('value'),\n",
    "            # dedicated columns for specific measurements\n",
    "            'voltage':          status_map.get('cur_voltage'),   # raw: divide by 10 for volts\n",
    "            'current':          status_map.get('cur_current'),   # raw: divide by 1000 for amps\n",
    "            'power':            status_map.get('cur_power'),     # raw: divide by 10 for watts\n",
    "            'temperature_c':    status_map.get('local_temperature'),\n",
    "        })\n",
    "\n",
    "    return pd.Series({\n",
    "        'source':           'unknown',\n",
    "        'extracted_code':   None,\n",
    "        'extracted_value':  None,\n",
    "        'voltage':          None,\n",
    "        'current':          None,\n",
    "        'power':            None,\n",
    "        'temperature_c':    None,\n",
    "    })\n",
    "\n",
    "\n",
    "print(\"\\nParsing payload column (15,000 rows)...\")\n",
    "parsed    = df_events['payload'].apply(parse_payload)\n",
    "df_events = pd.concat([df_events, parsed], axis=1)\n",
    "\n",
    "# Parse timestamp\n",
    "df_events['event_ts'] = pd.to_datetime(df_events['event_ts'], utc=True)\n",
    "df_events['date']     = df_events['event_ts'].dt.date\n",
    "\n",
    "# Convert extracted_value to numeric when possible\n",
    "# Booleans (True/False) become 1/0 — meaningful for switch/sensor events\n",
    "df_events['extracted_value_numeric'] = pd.to_numeric(\n",
    "    df_events['extracted_value'].apply(\n",
    "        lambda x: int(x) if isinstance(x, bool) else x\n",
    "    ),\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Scale raw Tuya electrical values to proper units\n",
    "df_events['voltage_v'] = df_events['voltage'] / 10\n",
    "df_events['current_a'] = df_events['current'] / 1000\n",
    "df_events['power_w']   = df_events['power']   / 10\n",
    "\n",
    "print(\"✓ Events cleaned:\", df_events.shape)\n",
    "print(\"\\nPayload source breakdown:\")\n",
    "print(df_events['source'].value_counts().to_string())\n",
    "print(\"\\nTop extracted codes:\")\n",
    "print(df_events['extracted_code'].value_counts().head(10).to_string())\n",
    "print(\"\\nAll unique extracted codes:\")\n",
    "print(df_events[\"extracted_code\"].value_counts().to_string())\n",
    "print(\"\\nDedicated measurement columns:\")\n",
    "print(f\"  voltage_v readings:  {df_events['voltage_v'].notna().sum()}\")\n",
    "print(f\"  current_a readings:  {df_events['current_a'].notna().sum()}\")\n",
    "print(f\"  power_w readings:    {df_events['power_w'].notna().sum()}\")\n",
    "print(f\"  temperature_c readings: {df_events['temperature_c'].notna().sum()}\")\n",
    "\n",
    "# verify devices and network\n",
    "df_check = df_events.merge(\n",
    "    df_devices[[\"device_id\", \"network\"]], on=\"device_id\", how=\"left\"\n",
    ")\n",
    "print(df_check.groupby([\"network\", \"source\"]).size())\n",
    "\n",
    "# FINAL SUMMARY\n",
    "print(\"\\n─── Final Dataset Summary ───\")\n",
    "print(f\"Users:   {df_users.shape[0]} rows | Regions: {sorted(df_users['region'].unique())}\")\n",
    "print(f\"Devices: {df_devices.shape[0]} rows | Networks: {df_devices['network'].unique().tolist()}\")\n",
    "print(f\"Events:  {df_events.shape[0]} rows | Date range: {df_events['date'].min()} → {df_events['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualizations\n",
    "Produce at least 3 meaningful charts, such as:\n",
    "- Event volume over time\n",
    "- Events per device or per user\n",
    "- Comparison between Ayla vs Tuya devices\n",
    "- Highlight any anomalies or interesting patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 – Hypotheses & Questions\n",
    "\n",
    "1. Propose 2–3 hypotheses about user or device behavior.\n",
    "2. Show how you would test each hypothesis with the available data.\n",
    "3. Clearly state whether the data supports, partially supports, or does not support the hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for hypothesis testing here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 – Reflection\n",
    "\n",
    "- What additional data would improve this analysis?\n",
    "- What limitations prevent deeper insights?\n",
    "- What would you explore next if this were Phase 0 of a larger project?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your reflection here...*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
